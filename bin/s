#!/usr/bin/env python
#
# Ollama LLM chat search
# Adapted from https://gist.github.com/Sh4yy/3941bf5014bc8c980fad797d85149b65

import os, argparse, asyncio, sys, platform
import ollama
from ollama import AsyncClient

if os.getenv('OLLAMA_HOST') is None:
	print('No \"OLLAMA_HOST\" set. Try:\nexport OLLAMA_HOST=\"http://localhost:11434\"')
	exit(1)

ollamaClient = AsyncClient(host=os.getenv('OLLAMA_HOST'))
defaultModelName = os.getenv('OLLAMA_DEFAULT_MODEL', 'phi3:latest')

def getInput(message, includeCarriageReturn=False):
	carriageReturn = '\n\n'
	return input(f"{message}:{carriageReturn if includeCarriageReturn else ' '}")

def getQuery():
	return getInput('Please provide a query', True)

async def chat(modelName, query, skip_prompt = False):
	print(f'\nUsing {modelName}...\n')
	messages = []
	if not skip_prompt:
		messages.append({
			'role': 'system',
			'content': f"""
				You are a CLI code generator. Respond with the CLI query to generate the code with only one short sentence description in first line.
				If the user asks for a specific language, respond with the CLI query to generate the code in that language.
				If CLI query is multiple lines, separate each line with a newline character.
				Do not write any markdown. Do not write any code.
				System Info: OS: {platform.platform()}
				First line is the description in one sentence.
				Example output:
				Installing a node package

				npm install browser-sync
			"""
		})
	messages.append({
		'role': 'user',
		'content': query
	})
	async for part in await ollamaClient.chat(model=modelName, messages=messages, stream=True):
		print(part['message']['content'], end='', flush=True)
	print('\n')

async def listModels():
	availableModels = (await ollamaClient.list())['models']
	raw_table = [['', '#', 'model', 'params', 'size']]
	raw_table.append(list(map(lambda cell: ''.join(['-'] * len(cell)), raw_table[0])))
	for i, model in enumerate(availableModels):
		raw_table.append(['*' if model['name'] == defaultModelName else '', i + 1, model['name'], model['details']['parameter_size'], f"{'{:.1f}'.format(float(model['size']) / 10**9)}gb"])
	tableString = ''
	for row in raw_table:
		tableString += '{:1} {:1} {: <25} {:<10} {:<5}'.format(*row) + '\n'
	print(tableString)
	return availableModels

async def getModelName():
	availableModels = await listModels()
	modelInput = getInput('Choose a model')
	if len(modelInput) > 0:
		modelIndex = int(modelInput) - 1
		return availableModels[modelIndex]['name']
	else:
		return None

async def main():
	parser = argparse.ArgumentParser(description='Ollama LLM chat search')
	sub_parsers = parser.add_subparsers(dest='command', title='commands')

	parser_chat = sub_parsers.add_parser('c', help='chat', description='Provide a search query')
	parser_chat.add_argument('-m', '--model', action='store_true', help='choose a model')
	parser_chat.add_argument('-np', '--no-prompt', action='store_true', help='no prompt')

	parser_list = sub_parsers.add_parser('list', help='list models')

	args, unknownArgs = parser.parse_known_args()

	try:
		workingModelName = defaultModelName
		await ollamaClient.show(workingModelName) # checks for default model

		if args.command == 'list':
			await listModels()
			sys.exit(0)
		elif args.command == 'c':
			if args.model:
				workingModelName = await getModelName()
			if len(unknownArgs) > 0:
				query = ' '.join(unknownArgs)
			else:
				query = getQuery()
			await chat(workingModelName, query, args.no_prompt)
		else:
			parser.print_help(sys.stderr)
			sys.exit(0)
	except ollama.ResponseError as e:
		print('Error:', e.error)
		if e.status_code == 404:
			print(f'Try running: ollama pull {workingModelName}')
			exit(1)
try:
	asyncio.run(main())
except (KeyboardInterrupt, EOFError):
	print('\nForce exited')
	exit(1)
