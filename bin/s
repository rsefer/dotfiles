#!/usr/bin/env python
#
# Ollama LLM chat search
# Adapted from https://gist.github.com/Sh4yy/3941bf5014bc8c980fad797d85149b65

import sys, platform
import ollama

workingModelName = 'llama3:latest'

def getInput(message, includeCarriageReturn=False):
	carriageReturn = '\n\n'
	return input(f"{message}:{carriageReturn if includeCarriageReturn else ' '}")

def getCommand():
	return getInput('Please provide a command', True)

args = sys.argv

if len(args) == 1:
	query = getCommand()
elif len(args) >= 2 and args[1] == '-m':
	availableModels = ollama.list()['models']
	raw_table = [
		['', '#', 'model', 'params'],
		['', '-', '-----', '------']
	]
	for i, model in enumerate(availableModels):
		raw_table.append(['*' if model['name'] == workingModelName else '', i + 1, model['name'], model['details']['parameter_size']])
	tableString = ''
	for row in raw_table:
		tableString += '{:1} {:1} {: >15} {: >6}'.format(*row) + '\n'
	print(tableString)
	modelInput = getInput('Choose a model')
	if len(modelInput) > 0:
		modelIndex = int(modelInput) - 1
		workingModelName = availableModels[modelIndex]['name']
	print(f'\nUsing {workingModelName}\n')
	query = getCommand()
else:
	query = ' '.join(args[1:])

try:
	stream = ollama.chat(
		model=workingModelName,
		messages=[
			{
				'role': 'system',
				'content': f"""
					You are a CLI code generator. Respond with the CLI command to generate the code with only one short sentence description in first line.
					If the user asks for a specific language, respond with the CLI command to generate the code in that language.
					If CLI command is multiple lines, separate each line with a newline character.
					Do not write any markdown. Do not write any code.
					System Info: OS: {platform.platform()}
					First line is the description in one sentence.
					Example output:
					Installing a node package

					npm install browser-sync
				"""
			},
			{
				'role': 'user',
				'content': query
			}
		],
		stream=True
	)
	for chunk in stream:
		print(chunk['message']['content'], end='', flush=True)
	print('\n')
except ollama.ResponseError as e:
	print('Error:', e.error)
	if e.status_code == 404:
		print(f'try: ollama pull {workingModelName}')
